{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209fc6bf",
   "metadata": {},
   "source": [
    "### KoChatGPT 만들기\n",
    "\n",
    "Foundational model 변경: gpu 메모리 및 이슈로 gpt-trinity 1.2B 사용해보기\n",
    "\n",
    "이슈: torch 버전도 안맞고 뭐도 안맞고 아니 뭐 하나 할때마다 버전 충돌이 자꾸 일어남\n",
    "\n",
    "심지어 아이펠에서 했는데, 모델 좀 바꿔보고 하려고 했더니만 당장은 뭐가 안되네?\n",
    "\n",
    "일단 보류\n",
    "\n",
    "플로우만이라도 최종본에 따보자\n",
    "\n",
    "회고:\n",
    "\n",
    "이게 이론상 RLHF를 적용해서 뭔가 파인튜닝도 하고, 특히 사람의 피드백이 들어가는 걸 체험할 수 있으니 좋을거라고 생각하는데\n",
    "\n",
    "실제로는 강화학습 도입에 대해서 중간에 큰 갭이 있어서 코드 구현이든 뭐든 따라가기 갑자기 벅찬 느낌이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f13d8",
   "metadata": {},
   "source": [
    "#### 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b456dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "Found existing installation: torch 1.9.1+cu111\n",
      "Uninstalling torch-1.9.1+cu111:\n",
      "  Successfully uninstalled torch-1.9.1+cu111\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Collecting torch==1.13.1+cu116\n",
      "  Using cached https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp39-cp39-linux_x86_64.whl (1977.9 MB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch==1.13.1+cu116) (4.7.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "Installing collected packages: torch\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1 requires pillow!=8.3.*,>=5.3.0, but you have pillow 8.3.2 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.13.1+cu116\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "Requirement already satisfied: transformers==4.28.1 in /opt/conda/lib/python3.9/site-packages (4.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1) (0.29.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1) (4.62.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1) (2021.11.10)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1) (1.21.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (4.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (2025.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.28.1) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1) (2.0.8)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "Requirement already satisfied: colossalai==0.2.7 in /opt/conda/lib/python3.9/site-packages (0.2.7)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from colossalai==0.2.7) (5.8.0)\n",
      "Requirement already satisfied: pre-commit in /opt/conda/lib/python3.9/site-packages (from colossalai==0.2.7) (3.3.3)\n",
      "Requirement already satisfied: contexttimer in /opt/conda/lib/python3.9/site-packages (from colossalai==0.2.7) (0.3.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from colossalai==0.2.7) (1.21.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from colossalai==0.2.7) (1.13.1+cu116)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.9/site-packages (from colossalai==0.2.7) (13.4.2)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from colossalai==0.2.7) (1.11.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from colossalai==0.2.7) (21.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from colossalai==0.2.7) (8.0.3)\n",
      "Requirement already satisfied: fabric in /opt/conda/lib/python3.9/site-packages (from colossalai==0.2.7) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from colossalai==0.2.7) (4.62.3)\n",
      "Requirement already satisfied: decorator>=5 in /opt/conda/lib/python3.9/site-packages (from fabric->colossalai==0.2.7) (5.1.1)\n",
      "Requirement already satisfied: paramiko>=2.4 in /opt/conda/lib/python3.9/site-packages (from fabric->colossalai==0.2.7) (3.2.0)\n",
      "Requirement already satisfied: invoke>=2.0 in /opt/conda/lib/python3.9/site-packages (from fabric->colossalai==0.2.7) (2.1.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->colossalai==0.2.7) (3.0.6)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai==0.2.7) (20.23.1)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai==0.2.7) (2.5.24)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai==0.2.7) (3.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai==0.2.7) (6.0)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai==0.2.7) (1.8.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from rich->colossalai==0.2.7) (2.15.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.9/site-packages (from rich->colossalai==0.2.7) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->colossalai==0.2.7) (4.7.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->colossalai==0.2.7) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from nodeenv>=0.11.1->pre-commit->colossalai==0.2.7) (59.4.0)\n",
      "Requirement already satisfied: pynacl>=1.5 in /opt/conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (1.5.0)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /opt/conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (4.0.1)\n",
      "Requirement already satisfied: cryptography>=3.3 in /opt/conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (41.0.1)\n",
      "Requirement already satisfied: filelock<4,>=3.12 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.12.2)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (0.3.6)\n",
      "Requirement already satisfied: platformdirs<4,>=3.5.1 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.8.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.9/site-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (2.21)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "fatal: destination path 'KoChatGPT' already exists and is not an empty directory.\n",
      "/aiffel/KoChatGPT/colossalai_ChatGPT_230319\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "Processing /aiffel/KoChatGPT/colossalai_ChatGPT_230319\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.20.1 in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (4.28.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (4.62.3)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (2.13.1)\n",
      "Requirement already satisfied: loralib in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (0.1.1)\n",
      "Requirement already satisfied: colossalai>=0.2.4 in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (0.2.7)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (1.13.1+cu116)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (0.0.113)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (21.3)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (13.4.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (8.0.3)\n",
      "Requirement already satisfied: fabric in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.21.4)\n",
      "Requirement already satisfied: contexttimer in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (0.3.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (5.8.0)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.11.1)\n",
      "Requirement already satisfied: pre-commit in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.3.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.29.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2.26.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2021.11.10)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (12.0.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (2025.3.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (2.0.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (3.8.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (1.3.3)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (0.3.4)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (0.70.12.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt==0.1.0) (8.2.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt==0.1.0) (0.5.8)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt==0.1.0) (1.4.48)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt==0.1.0) (1.10.9)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (4.7.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (4.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.7.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (2.0.8)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (1.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->colossalai>=0.2.4->chatgpt==0.1.0) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (1.26.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.9/site-packages (from SQLAlchemy<2,>=1->langchain->chatgpt==0.1.0) (2.0.2)\n",
      "Requirement already satisfied: invoke>=2.0 in /opt/conda/lib/python3.9/site-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.1.3)\n",
      "Requirement already satisfied: decorator>=5 in /opt/conda/lib/python3.9/site-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (5.1.1)\n",
      "Requirement already satisfied: paramiko>=2.4 in /opt/conda/lib/python3.9/site-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (3.2.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->chatgpt==0.1.0) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->chatgpt==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (20.23.1)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (2.5.24)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (1.8.0)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.9/site-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->colossalai>=0.2.4->chatgpt==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (59.4.0)\n",
      "Requirement already satisfied: pynacl>=1.5 in /opt/conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: cryptography>=3.3 in /opt/conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (41.0.1)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /opt/conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (4.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets->chatgpt==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (0.4.3)\n",
      "Requirement already satisfied: platformdirs<4,>=3.5.1 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.8.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (0.3.6)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.9/site-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.21)\n",
      "Building wheels for collected packages: chatgpt\n",
      "  Building wheel for chatgpt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for chatgpt: filename=chatgpt-0.1.0-py3-none-any.whl size=46664 sha256=8d666fd7056587c924d26816e46adc50e06b66635074d657ebb25df2987e6ae0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-buox2twm/wheels/fc/c3/93/79adb4a5435789558ae28b46e23f406195eef0314a40af8a6e\n",
      "Successfully built chatgpt\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "Installing collected packages: chatgpt\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "Successfully installed chatgpt-0.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "/aiffel\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "Requirement already satisfied: openai in /opt/conda/lib/python3.9/site-packages (0.27.8)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from openai) (4.62.3)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.9/site-packages (from openai) (2.26.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20->openai) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20->openai) (2.0.8)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai) (5.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai) (4.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.9/site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->openai) (4.7.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "Requirement already satisfied: langchain==0.0.113 in /opt/conda/lib/python3.9/site-packages (0.0.113)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.9/site-packages (from langchain==0.0.113) (3.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain==0.0.113) (1.21.4)\n",
      "Requirement already satisfied: PyYAML<7,>=6 in /opt/conda/lib/python3.9/site-packages (from langchain==0.0.113) (6.0)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain==0.0.113) (1.10.9)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.9/site-packages (from langchain==0.0.113) (2.26.0)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/lib/python3.9/site-packages (from langchain==0.0.113) (0.5.8)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain==0.0.113) (1.4.48)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.9/site-packages (from langchain==0.0.113) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (21.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (2.0.8)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (4.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (5.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.7.2)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.9/site-packages (from pydantic<2,>=1->langchain==0.0.113) (4.7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.113) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.113) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.113) (2.10)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.9/site-packages (from SQLAlchemy<2,>=1->langchain==0.0.113) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (21.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (0.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (3.0.6)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch -y\n",
    "!pip install torch==1.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install transformers==4.28.1\n",
    "!pip install colossalai==0.2.7\n",
    "!git clone https://github.com/airobotlab/KoChatGPT\n",
    "!mv KoChatGPT/data_kochatgpt .\n",
    "!mv KoChatGPT/img .\n",
    "%cd KoChatGPT/colossalai_ChatGPT_230319/\n",
    "!pip install .\n",
    "%cd ../../\n",
    "!pip install openai\n",
    "!pip install langchain==0.0.113\n",
    "!pip install pandas>=1.4.1\n",
    "\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017a874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
    "from copy import deepcopy\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, BloomTokenizerFast\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28e33f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data_path_1_SFT='./data_kochatgpt/kochatgpt_1_SFT.jsonl', model_name='skt/kogpt2-base-v2', max_epochs=1, train_batch_size=8, output_dir='./output_1_SFT')\n"
     ]
    }
   ],
   "source": [
    "# parser 만들기\n",
    "ps = argparse.ArgumentParser()\n",
    "ps.add_argument('--data_path_1_SFT', type=str, default='./data_kochatgpt/kochatgpt_1_SFT.jsonl')\n",
    "ps.add_argument('--model_name', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
    "ps.add_argument('--max_epochs', type=int, default=2)\n",
    "ps.add_argument('--train_batch_size', type=int, default=8)\n",
    "ps.add_argument('--output_dir', type=str, default='./output_1_SFT')\n",
    "\n",
    "args = ps.parse_args(args=[])\n",
    "\n",
    "# for test\n",
    "args.model_name = 'skt/kogpt2-base-v2'\n",
    "args.max_epochs = 1\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb8cd4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b49bd2c32843a79d054b65fb2c0324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62419537f944526ad4c15ff44653062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f979b365368413e824db5f463107a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 자랑스런 세계인”이라며 “앞으로도 한국이 더 많은 것을 배우고 배울 수 있도록 노력하겠다”고 말했다.</d> 서울중앙지검 특수1부(부장검사 임관혁)는 이날 오전 10시30분부터 오후 1시까지 김 전 차관을 피의자 신분으로 불러 조사하고 있다.\n",
      "김씨는 지난해 11월 검찰에 출석하면서 \"검찰에서 (박근혜) 대통령과의 관계를 잘 모른다고 진술했다\"고 말한 바 있다. 만약 박 대통령이 김씨에게 '최순실 게이트' 관련 내용을 얘기했다면 이는 거짓말이다.\n",
      "하지만 이 같은 사실이 알려지면서 청와대 측은 사실무근의 입장을 밝혔다.\n",
      "청원경찰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': '0 : **는 게임 좋아하니\\n1 : ***는 게임을 싫어하니\\n'}],\n",
       " [{'generated_text': '0 : 어제 강남에서 살인사건 났대 ㅜㅜ 너무 무서워\\n1 : 헐 왜? 무슨 일 있었어?\\n0 : 사진보니까 막 피흘리는 사람있고 경찰들이 떠서 제압하고 난리도 아니었다던데??\\n1 : 그거 뭐야?!!!!!\\n2 : 아, 그런 거 없잖아.\\n3 : 다짜고짜...\\n3 : 아니야, 이게 진짜 너희들 장난이야?!!!\\n4 : 이건 또 누가 봐도 정신나간짓이야!!!\\n5 : 안돼~!'}],\n",
       " [{'generated_text': '0 : 자기야 어제는 나한테 왜 그랬어?\\n1 : 뭔 일 있었어?\\n0 : 어떻게 나한테 말도 없이 그럴 수 있어? 나 진짜 실망했어\\n1 : 뭘 잘못했어?\\n2 : 뭐가 잘못됐어?\\n3 : 뭔 일이 있었어?\\n4 : 뭐가 잘못된 거야?\\n5 : 무슨 일이 있었던 거야?\\n6 : 뭐라고 했어?\\n7 : 아무 일도 없었어?\\n8 : 뭐라고 한 거야?\\n9 : 뭐라고 했는데'}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test & load skt gpt2 kroean\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel \n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "                                                    pad_token='<pad>', mask_token='<mask>')\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "text = '나는 자랑스런 세계인'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "gen_ids = model.generate(input_ids,\n",
    "                         max_length=128,\n",
    "                         repetition_penalty=2.0,\n",
    "                         pad_token_id=tokenizer.pad_token_id,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         bos_token_id=tokenizer.bos_token_id,\n",
    "                         use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "generator(\n",
    "    [\"0 : **는 게임 좋아하니\\n1 :\",\n",
    "    \"0 : 어제 강남에서 살인사건 났대 ㅜㅜ 너무 무서워\\n1 : 헐 왜? 무슨 일 있었어?\\n0 : 사진보니까 막 피흘리는 사람있고 경찰들이 떠서 제압하고 난리도 아니었다던데??\\n1 :\",\n",
    "    \"0 : 자기야 어제는 나한테 왜 그랬어?\\n1 : 뭔 일 있었어?\\n0 : 어떻게 나한테 말도 없이 그럴 수 있어? 나 진짜 실망했어\\n1 : \"],\n",
    "    **generation_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81df8a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 <pad>\n",
      "1 </s>\n",
      "1 </s>\n",
      "5 <unk>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id, tokenizer.pad_token)\n",
    "print(tokenizer.eos_token_id, tokenizer.eos_token)\n",
    "print(tokenizer.bos_token_id, tokenizer.bos_token)\n",
    "print(tokenizer.unk_token_id, tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59892d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data config\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"<pad>\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\" # \n",
    "DEFAULT_BOS_TOKEN = \"</s>\" # 위랑 같음\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\" \n",
    "\n",
    "# setting below\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\"명령어:\\n{prompt}\\n\\n### 입력:\\n{input}\\n\\n### 응답:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\"명령어:\\n{prompt}\\n\\n### 응답:\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "687cf32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=256, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "## 모델 준비\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    args.model_name,\n",
    "    padding_side=\"right\", # 오른쪽을 채우기\n",
    "    model_max_length=256,    \n",
    ")\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "        \"pad_token\": DEFAULT_PAD_TOKEN,\n",
    "    }\n",
    ")    \n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b26efd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## data check ##\n",
      "{'prompt': '불고기용 고기 한우에요?', 'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\", 'tokens': 193}\n",
      "명령어:\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### 응답:\n",
      "'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.</s>\n",
      "Tokenizing inputs... This may take some time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    }
   ],
   "source": [
    "## prepare data\n",
    "from typing import Optional, Dict, Sequence\n",
    "    \n",
    "class SFT_dataset(Dataset):\n",
    "    '''SFT dataset by wygo'''\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=True):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        \n",
    "        ## format\n",
    "        pattern_instruction = 'prompt'\n",
    "        pattern_input = 'input'\n",
    "        pattern_output = 'completion' \n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "            if verbose:\n",
    "                print('## data check ##')\n",
    "                print((list_data_dict[0]))\n",
    "                \n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # 템플릿 가져오기\n",
    "\n",
    "        # 입력\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            if example.get(pattern_input, \"\") != \"\":\n",
    "                tmp = prompt_input.format_map(example)\n",
    "            else:\n",
    "                tmp = prompt_no_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "            \n",
    "        # 출력\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "\n",
    "        if verbose:\n",
    "            idx = 0\n",
    "            print((sources[idx]))\n",
    "            print((targets[idx]))\n",
    "            print(\"Tokenizing inputs... This may take some time...\")\n",
    "\n",
    "    \n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source만\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = IGNORE_INDEX  # source 부분은 -100으로 채운다\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)        \n",
    "        \n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))    \n",
    "        \n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        \"\"\"Tokenize a list of strings.\"\"\"\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "\n",
    "train_dataset = SFT_dataset(data_path_1_SFT=args.data_path_1_SFT, tokenizer=tokenizer)\n",
    "eval_dataset  = None \n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3ce6b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (0.20.3)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.5.2-py3-none-any.whl (345 kB)\n",
      "     |████████████████████████████████| 345 kB 8.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate) (6.0)\n",
      "Collecting torch>=2.0.0\n",
      "  Downloading torch-2.6.0-cp39-cp39-manylinux1_x86_64.whl (766.7 MB)\n",
      "     |████████████████████████████████| 766.7 MB 1.9 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.9/site-packages (from accelerate) (0.29.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.9/site-packages (from accelerate) (1.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.9/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate) (5.8.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.26.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.7.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.62.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->accelerate) (3.0.6)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "     |████████████████████████████████| 207.5 MB 7.9 kB/s              \n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "     |████████████████████████████████| 127.9 MB 52 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (2.6.3)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "     |████████████████████████████████| 664.8 MB 2.7 kB/s              \n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "     |████████████████████████████████| 150.1 MB 2.5 kB/s             \n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "     |████████████████████████████████| 21.1 MB 29 kB/s              \n",
      "\u001b[?25hCollecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "     |████████████████████████████████| 363.4 MB 1.6 kB/s              \n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "     |████████████████████████████████| 211.5 MB 12 kB/s              \n",
      "\u001b[?25hCollecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "     |████████████████████████████████| 6.2 MB 89.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from torch>=2.0.0->accelerate) (3.0.3)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "     |████████████████████████████████| 13.8 MB 59.2 MB/s            \n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "     |████████████████████████████████| 99 kB 12.9 MB/s            \n",
      "\u001b[?25hCollecting triton==3.2.0\n",
      "  Downloading triton-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "     |████████████████████████████████| 253.1 MB 21 kB/s              \n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "     |████████████████████████████████| 24.6 MB 34.0 MB/s            \n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "     |████████████████████████████████| 883 kB 90.7 MB/s            \n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "     |████████████████████████████████| 188.7 MB 7.7 kB/s              \n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "     |████████████████████████████████| 56.3 MB 111 kB/s             /s eta 0:00:01\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     |████████████████████████████████| 536 kB 57.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-cusparse-cu12, nvidia-cublas-cu12, mpmath, typing-extensions, triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparselt-cu12, nvidia-cusolver-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, torch, accelerate\n",
      "  Attempting uninstall: typing-extensions\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "    Found existing installation: typing-extensions 4.7.0\n",
      "    Uninstalling typing-extensions-4.7.0:\n",
      "      Successfully uninstalled typing-extensions-4.7.0\n",
      "  Attempting uninstall: torch\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "    Found existing installation: torch 1.13.1+cu116\n",
      "    Uninstalling torch-1.13.1+cu116:\n",
      "      Successfully uninstalled torch-1.13.1+cu116\n",
      "  Attempting uninstall: accelerate\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "    Found existing installation: accelerate 0.20.3\n",
      "    Uninstalling accelerate-0.20.3:\n",
      "      Successfully uninstalled accelerate-0.20.3\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1 requires pillow!=8.3.*,>=5.3.0, but you have pillow 8.3.2 which is incompatible.\n",
      "tensorflow-gpu 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.4 which is incompatible.\n",
      "tensorflow-gpu 2.6.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "tensorflow-gpu 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 4.12.2 which is incompatible.\u001b[0m\n",
      "Successfully installed accelerate-1.5.2 mpmath-1.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0 typing-extensions-4.12.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bebd586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 09:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.144200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.939900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.875700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.738400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.659100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_steps = 3,\n",
    "    save_steps=500,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_state()\n",
    "\n",
    "def save_model(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)\n",
    "\n",
    "save_model(trainer=trainer, output_dir=args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1418dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel\n"
     ]
    }
   ],
   "source": [
    "cd aiffel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4655f845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'=1.4.1'           Dockerfile   \u001b[0m\u001b[01;34moutput_1_SFT\u001b[0m/           setup.py\r\n",
      " \u001b[01;34maiffel\u001b[0m/           \u001b[01;34mexamples\u001b[0m/    pytest.ini              \u001b[01;34mtest\u001b[0m/\r\n",
      " \u001b[01;34mbenchmarks\u001b[0m/       \u001b[01;34mimg\u001b[0m/         README.md               \u001b[01;34mtests\u001b[0m/\r\n",
      " \u001b[01;34mchatgpt\u001b[0m/          \u001b[01;34mKoChatGPT\u001b[0m/   requirements-test.txt   Untitled.ipynb\r\n",
      " \u001b[01;34mdata_kochatgpt\u001b[0m/   LICENSE      requirements.txt        version.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f94319d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "completion: 명령어:\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### 응답:'저는 AI 어시스턴트이기 때문에 고기를 먹을 수 없습니다. 하지만 일반적으로 불고기는 건강에 좋은 식품 중 하나이기 때문에 쇠고기를 먹는 것은 위험하지 않습니다. 그러나 일부 음식점에서는 불고기를 활용한 다양한 메뉴를 제공하고 있으니 참고하시기 바랍니다. important provide\n",
      "######################################################################\n",
      "completion: 명령어:\n",
      "리처드 닉슨이 43대 부통령직을 수행한 연도는?\n",
      "\n",
      "### 응답:'리처드 닉슨은 40대 부통령직을 수행했습니다. Now what you please provide more context. \"리처드 닉슨\"이 어떤 인물인지 명확하지 않아 정확한 답변을 드리기 어렵습니다. 추가적인 정보를 제공해주시면 더 정확한 답\n",
      "######################################################################\n",
      "completion: 명령어:\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### 응답:'시카고 오 헤어 국제공항은 미국 캘리포니아주 샌프란시스코에 위치해 있습니다. Canada OpenAir 국제공항, New York California 국제공항 등 다양한 국제공항이 운영되고 있습니다. National Geographic Center of Translation Systems, Director for the Commission of Korea\n",
      "######################################################################\n",
      "completion: 명령어:\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### 응답:'저는 인공지능 챗봇이므로 미세먼지 정보를 알 수 없습니다. 하지만 일반적으로 미세먼지 농도는 매우 높기 때문에 외출 시 반드시 마스크를 착용하시는 것이 좋습니다. 또한, 미세먼지 농도가 높은 날에는 야외활동을 자제하는 것이 좋습니다. SNS 계정에서 미세먼지 농도를\n"
     ]
    }
   ],
   "source": [
    "## 모델 불러오기\n",
    "if True:\n",
    "  model = GPT2LMHeadModel.from_pretrained('./output_1_SFT')\n",
    "model.eval()\n",
    "\n",
    "## 추론 테스트\n",
    "with torch.no_grad():\n",
    "  generator = pipeline('text-generation', model=args.output_dir, tokenizer=tokenizer)\n",
    "  # generator = pipeline('text-generation', model=model.cpu(), tokenizer=tokenizer, config={'max_length':800})\n",
    "\n",
    "  generation_args = dict(\n",
    "      num_beams=4,\n",
    "      repetition_penalty=2.0,\n",
    "      no_repeat_ngram_size=4,\n",
    "      eos_token_id=375, # \\n\n",
    "      max_new_tokens=64,\n",
    "      do_sample=True,\n",
    "      top_k=50,\n",
    "      early_stopping=True\n",
    "  )\n",
    "\n",
    "  list_prompt = ['불고기용 고기 한우에요?',\n",
    "                '리처드 닉슨이 43대 부통령직을 수행한 연도는?',\n",
    "                '시카고 오헤어 국제공항은 어디에 있어',\n",
    "                '오늘 미세먼지 어때?']\n",
    "  list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "  list_result = generator(list_prompt, **generation_args)\n",
    "  for prompt, result in zip(list_prompt, list_result):\n",
    "      print(('#'*70))\n",
    "      print(('completion: %s'%(result[0]['generated_text'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e752c549",
   "metadata": {},
   "source": [
    "___\n",
    "학습완료! 256 토큰 내에서 어느정도 사람의 질문에 대한 답변을 꽤 잘 해내는 것 같은데?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c669e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 모델 학습을 위해 할당한 GPU 메모리 초기화\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c463bdc",
   "metadata": {},
   "source": [
    "### RM 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9e03112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "Processing ./aiffel/KoChatGPT/colossalai_ChatGPT_230319\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.20.1 in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (4.28.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (4.62.3)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (2.13.1)\n",
      "Requirement already satisfied: loralib in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (0.1.1)\n",
      "Requirement already satisfied: colossalai>=0.2.4 in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (0.2.7)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (2.6.0)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (0.0.113)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.21.4)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (8.0.3)\n",
      "Requirement already satisfied: pre-commit in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.3.3)\n",
      "Requirement already satisfied: fabric in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.1.0)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.11.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (21.3)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (13.4.2)\n",
      "Requirement already satisfied: contexttimer in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (0.3.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (5.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2021.11.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (3.12.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.29.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2.26.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (0.3.4)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (0.70.12.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (2.0.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (2025.3.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (1.3.3)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (12.0.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt==0.1.0) (8.2.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt==0.1.0) (0.5.8)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt==0.1.0) (1.4.48)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt==0.1.0) (1.10.9)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (11.6.1.9)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (4.12.2)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (11.2.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (12.3.1.170)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (2.6.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (12.4.5.8)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (3.0.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from sympy==1.13.1->torch->chatgpt==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (5.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (2.0.8)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (4.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (21.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (3.19.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (1.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->colossalai>=0.2.4->chatgpt==0.1.0) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.9/site-packages (from SQLAlchemy<2,>=1->langchain->chatgpt==0.1.0) (2.0.2)\n",
      "Requirement already satisfied: paramiko>=2.4 in /opt/conda/lib/python3.9/site-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (3.2.0)\n",
      "Requirement already satisfied: invoke>=2.0 in /opt/conda/lib/python3.9/site-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.1.3)\n",
      "Requirement already satisfied: decorator>=5 in /opt/conda/lib/python3.9/site-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (5.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->torch->chatgpt==0.1.0) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->chatgpt==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->chatgpt==0.1.0) (2021.3)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (1.8.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (2.5.24)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.1)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (20.23.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.9/site-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->colossalai>=0.2.4->chatgpt==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (59.4.0)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /opt/conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (4.0.1)\n",
      "Requirement already satisfied: pynacl>=1.5 in /opt/conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: cryptography>=3.3 in /opt/conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (41.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets->chatgpt==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (0.4.3)\n",
      "Requirement already satisfied: platformdirs<4,>=3.5.1 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.8.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (0.3.6)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.9/site-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.21)\n",
      "Building wheels for collected packages: chatgpt\n",
      "  Building wheel for chatgpt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for chatgpt: filename=chatgpt-0.1.0-py3-none-any.whl size=46664 sha256=92afc11461857d6892e18783d3d882641166b5e32f6d919f088b0470517b38de\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0xlws0wt/wheels/79/25/c3/338e0c56a2253a8ea6c41e8692f6eb2409a3898c63b234b103\n",
      "Successfully built chatgpt\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "Installing collected packages: chatgpt\n",
      "  Attempting uninstall: chatgpt\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "    Found existing installation: chatgpt 0.1.0\n",
      "    Uninstalling chatgpt-0.1.0:\n",
      "      Successfully uninstalled chatgpt-0.1.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "Successfully installed chatgpt-0.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.9/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"$HOME/aiffel/KoChatGPT/colossalai_ChatGPT_230319/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3223c67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu116'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f45c871",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">9</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch.optim</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Adam                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">chatgpt.dataset</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> RewardDataset                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">chatgpt.models.base</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> RewardModel                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 9 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">chatgpt.trainer</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> RewardModelTrainer                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">chatgpt.trainer.strategies</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> NaiveStrategy                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">datasets</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> load_dataset                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">transformers</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/aiffel/chatgpt/trainer/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.base</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Trainer                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.ppo</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> PPOTrainer                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.rm</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> RewardModelTrainer                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/aiffel/chatgpt/trainer/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  8 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch.utils.data</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> DistributedSampler                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  9 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">tqdm</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> tqdm                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 11 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.callbacks</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Callback                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 12 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.strategies</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Strategy                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 13 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.utils</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> is_rank_0                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 14 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/aiffel/chatgpt/trainer/callbacks/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.base</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Callback                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.performance_evaluator</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> PerformanceEvaluator                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.save_checkpoint</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> SaveCheckpoint                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span>__all__ = [<span style=\"color: #808000; text-decoration-color: #808000\">'Callback'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">'PerformanceEvaluator'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">'SaveCheckpoint'</span>]                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/aiffel/chatgpt/trainer/callbacks/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">save_checkpoint.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">os</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch.distributed</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">dist</span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 4 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">chatgpt.trainer.strategies</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> ColossalAIStrategy, Strategy                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">chatgpt.trainer.utils</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> is_rank_0                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> nn                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch.optim</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Optimizer                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/aiffel/chatgpt/trainer/strategies/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.base</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Strategy                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.colossalai</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> ColossalAIStrategy                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.ddp</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> DDPStrategy                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.naive</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> NaiveStrategy                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/aiffel/chatgpt/trainer/strategies/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">colossalai.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">12</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  9 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">chatgpt.models.lora</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> LoraLinear                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch.optim</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Optimizer                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 11 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 12 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">colossalai</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 13 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">colossalai.nn.optimizer</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> CPUAdam, HybridAdam                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 14 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">colossalai.nn.parallel</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> ZeroDDP, zero_model_wrapper, zero_optim_wrapper         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 15 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">colossalai.nn.parallel.utils</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> get_static_torch_model                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/colossalai/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 1 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.initialize</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> (                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>get_default_parser,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>initialize,                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>launch,                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/colossalai/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">initialize.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">18</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 15 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch.optim.optimizer</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Optimizer                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 16 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch.utils.data</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> DataLoader                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 17 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 18 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">colossalai.amp</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> AMP_TYPE, convert_to_amp                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 19 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">colossalai.amp.naive_amp</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> NaiveAMPModel                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 20 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">colossalai.builder.builder</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> build_gradient_handler                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 21 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">colossalai.context</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Config, ConfigException, ParallelMode                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/colossalai/amp/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 # -*- encoding: utf-8 -*-</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.amp_type</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> AMP_TYPE                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 5 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">colossalai.context</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Config                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch.nn</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">nn</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch.optim</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Optimizer                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch.nn.modules.loss</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> _Loss                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/colossalai/context/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.config</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Config, ConfigException                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.parallel_context</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> ParallelContext                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.parallel_mode</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> ParallelMode                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.moe_context</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> MOE_CONTEXT                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.process_group_initializer</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> *                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/colossalai/context/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">parallel_context.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">17</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 14 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">colossalai.context.config</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> Config                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 15 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">colossalai.global_variables</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> tensor_parallel_env <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> env                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 16 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">colossalai.logging</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> get_dist_logger                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 17 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">colossalai.registry</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> DIST_GROUP_INITIALIZER                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 18 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 19 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.parallel_mode</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> ParallelMode                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 20 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.random</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> add_seed, get_seeds, set_mode                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/colossalai/registry/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 1 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch.distributed.optim</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">dist_optim</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch.nn</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">nn</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch.optim</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">optim</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/distributed/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">13</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> optim                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>13 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">.apply_optimizer_in_backward</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> (                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>_apply_optimizer_in_backward,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>_get_in_backward_optimizers,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span>)                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/distributed/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">apply_optimizer_in_backward.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">12</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  9 # without changing it's life-time.</span>                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 # NOTE: Alternative is to add the meta-data as an attribute to the tensor,</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 11 #       but that will serialize the meta-data if Tensor is serialized.</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 12 param_to_optim_hook_handle_map = torch.utils.weak.WeakTensorKeyDictionary()                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 13 </span>param_to_acc_grad_map = torch.utils.weak.WeakTensorKeyDictionary()                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 14 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 15 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span>module <span style=\"color: #008000; text-decoration-color: #008000\">'torch.utils'</span> has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'weak'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m9\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mtorch\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96moptim\u001b[0m \u001b[94mimport\u001b[0m Adam                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mchatgpt\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mdataset\u001b[0m \u001b[94mimport\u001b[0m RewardDataset                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mchatgpt\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mmodels\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mbase\u001b[0m \u001b[94mimport\u001b[0m RewardModel                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 9 \u001b[94mfrom\u001b[0m \u001b[4;96mchatgpt\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mtrainer\u001b[0m \u001b[94mimport\u001b[0m RewardModelTrainer                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mchatgpt\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mtrainer\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mstrategies\u001b[0m \u001b[94mimport\u001b[0m NaiveStrategy                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mdatasets\u001b[0m \u001b[94mimport\u001b[0m load_dataset                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mtransformers\u001b[0m \u001b[94mimport\u001b[0m AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/aiffel/chatgpt/trainer/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 \u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mbase\u001b[0m \u001b[94mimport\u001b[0m Trainer                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mppo\u001b[0m \u001b[94mimport\u001b[0m PPOTrainer                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mrm\u001b[0m \u001b[94mimport\u001b[0m RewardModelTrainer                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/aiffel/chatgpt/trainer/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m11\u001b[0m in \u001b[92m<module>\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  8 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mtorch\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mutils\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mdata\u001b[0m \u001b[94mimport\u001b[0m DistributedSampler                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  9 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mtqdm\u001b[0m \u001b[94mimport\u001b[0m tqdm                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 10 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 11 \u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mcallbacks\u001b[0m \u001b[94mimport\u001b[0m Callback                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 12 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mstrategies\u001b[0m \u001b[94mimport\u001b[0m Strategy                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 13 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mutils\u001b[0m \u001b[94mimport\u001b[0m is_rank_0                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 14 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/aiffel/chatgpt/trainer/callbacks/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m3\u001b[0m in \u001b[92m<module>\u001b[0m                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mbase\u001b[0m \u001b[94mimport\u001b[0m Callback                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mperformance_evaluator\u001b[0m \u001b[94mimport\u001b[0m PerformanceEvaluator                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3 \u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96msave_checkpoint\u001b[0m \u001b[94mimport\u001b[0m SaveCheckpoint                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m__all__ = [\u001b[33m'\u001b[0m\u001b[33mCallback\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33mPerformanceEvaluator\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33mSaveCheckpoint\u001b[0m\u001b[33m'\u001b[0m]                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m6 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/aiffel/chatgpt/trainer/callbacks/\u001b[0m\u001b[1;33msave_checkpoint.py\u001b[0m:\u001b[94m4\u001b[0m in \u001b[92m<module>\u001b[0m                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 1 \u001b[0m\u001b[94mimport\u001b[0m \u001b[4;96mos\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m\u001b[94mimport\u001b[0m \u001b[4;96mtorch\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mdistributed\u001b[0m \u001b[94mas\u001b[0m \u001b[4;96mdist\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 4 \u001b[94mfrom\u001b[0m \u001b[4;96mchatgpt\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mtrainer\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mstrategies\u001b[0m \u001b[94mimport\u001b[0m ColossalAIStrategy, Strategy                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mchatgpt\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mtrainer\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mutils\u001b[0m \u001b[94mimport\u001b[0m is_rank_0                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mtorch\u001b[0m \u001b[94mimport\u001b[0m nn                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mtorch\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96moptim\u001b[0m \u001b[94mimport\u001b[0m Optimizer                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/aiffel/chatgpt/trainer/strategies/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<module>\u001b[0m                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mbase\u001b[0m \u001b[94mimport\u001b[0m Strategy                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2 \u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mcolossalai\u001b[0m \u001b[94mimport\u001b[0m ColossalAIStrategy                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mddp\u001b[0m \u001b[94mimport\u001b[0m DDPStrategy                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mnaive\u001b[0m \u001b[94mimport\u001b[0m NaiveStrategy                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/aiffel/chatgpt/trainer/strategies/\u001b[0m\u001b[1;33mcolossalai.py\u001b[0m:\u001b[94m12\u001b[0m in \u001b[92m<module>\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  9 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mchatgpt\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mmodels\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mlora\u001b[0m \u001b[94mimport\u001b[0m LoraLinear                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 10 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mtorch\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96moptim\u001b[0m \u001b[94mimport\u001b[0m Optimizer                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 11 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 12 \u001b[94mimport\u001b[0m \u001b[4;96mcolossalai\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 13 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mcolossalai\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mnn\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96moptimizer\u001b[0m \u001b[94mimport\u001b[0m CPUAdam, HybridAdam                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 14 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mcolossalai\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mnn\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mparallel\u001b[0m \u001b[94mimport\u001b[0m ZeroDDP, zero_model_wrapper, zero_optim_wrapper         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 15 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mcolossalai\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mnn\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mparallel\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mutils\u001b[0m \u001b[94mimport\u001b[0m get_static_torch_model                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/colossalai/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 1 \u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96minitialize\u001b[0m \u001b[94mimport\u001b[0m (                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0m\u001b[2m│   \u001b[0mget_default_parser,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m\u001b[2m│   \u001b[0minitialize,                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m│   \u001b[0mlaunch,                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/colossalai/\u001b[0m\u001b[1;33minitialize.py\u001b[0m:\u001b[94m18\u001b[0m in \u001b[92m<module>\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 15 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mtorch\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96moptim\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96moptimizer\u001b[0m \u001b[94mimport\u001b[0m Optimizer                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 16 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mtorch\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mutils\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mdata\u001b[0m \u001b[94mimport\u001b[0m DataLoader                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 17 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 18 \u001b[94mfrom\u001b[0m \u001b[4;96mcolossalai\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mamp\u001b[0m \u001b[94mimport\u001b[0m AMP_TYPE, convert_to_amp                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 19 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mcolossalai\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mamp\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mnaive_amp\u001b[0m \u001b[94mimport\u001b[0m NaiveAMPModel                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 20 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mcolossalai\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mbuilder\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mbuilder\u001b[0m \u001b[94mimport\u001b[0m build_gradient_handler                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 21 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mcolossalai\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mcontext\u001b[0m \u001b[94mimport\u001b[0m Config, ConfigException, ParallelMode                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/colossalai/amp/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m5\u001b[0m in \u001b[92m<module>\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0m\u001b[2m# -*- encoding: utf-8 -*-\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mamp_type\u001b[0m \u001b[94mimport\u001b[0m AMP_TYPE                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 5 \u001b[94mfrom\u001b[0m \u001b[4;96mcolossalai\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mcontext\u001b[0m \u001b[94mimport\u001b[0m Config                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[94mimport\u001b[0m \u001b[4;96mtorch\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mnn\u001b[0m \u001b[94mas\u001b[0m \u001b[4;96mnn\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mtorch\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96moptim\u001b[0m \u001b[94mimport\u001b[0m Optimizer                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mtorch\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mnn\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mmodules\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mloss\u001b[0m \u001b[94mimport\u001b[0m _Loss                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/colossalai/context/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<module>\u001b[0m              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mconfig\u001b[0m \u001b[94mimport\u001b[0m Config, ConfigException                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2 \u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mparallel_context\u001b[0m \u001b[94mimport\u001b[0m ParallelContext                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mparallel_mode\u001b[0m \u001b[94mimport\u001b[0m ParallelMode                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mmoe_context\u001b[0m \u001b[94mimport\u001b[0m MOE_CONTEXT                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mprocess_group_initializer\u001b[0m \u001b[94mimport\u001b[0m *                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/colossalai/context/\u001b[0m\u001b[1;33mparallel_context.py\u001b[0m:\u001b[94m17\u001b[0m in \u001b[92m<module>\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 14 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mcolossalai\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mcontext\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mconfig\u001b[0m \u001b[94mimport\u001b[0m Config                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 15 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mcolossalai\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mglobal_variables\u001b[0m \u001b[94mimport\u001b[0m tensor_parallel_env \u001b[94mas\u001b[0m env                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 16 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mcolossalai\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mlogging\u001b[0m \u001b[94mimport\u001b[0m get_dist_logger                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 17 \u001b[94mfrom\u001b[0m \u001b[4;96mcolossalai\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mregistry\u001b[0m \u001b[94mimport\u001b[0m DIST_GROUP_INITIALIZER                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 18 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 19 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mparallel_mode\u001b[0m \u001b[94mimport\u001b[0m ParallelMode                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 20 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mrandom\u001b[0m \u001b[94mimport\u001b[0m add_seed, get_seeds, set_mode                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/colossalai/registry/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 1 \u001b[94mimport\u001b[0m \u001b[4;96mtorch\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mdistributed\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96moptim\u001b[0m \u001b[94mas\u001b[0m \u001b[4;96mdist_optim\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0m\u001b[94mimport\u001b[0m \u001b[4;96mtorch\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mnn\u001b[0m \u001b[94mas\u001b[0m \u001b[4;96mnn\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m\u001b[94mimport\u001b[0m \u001b[4;96mtorch\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96moptim\u001b[0m \u001b[94mas\u001b[0m \u001b[4;96moptim\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/distributed/optim/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m13\u001b[0m in \u001b[92m<module>\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[94mimport\u001b[0m \u001b[4;96mtorch\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mtorch\u001b[0m \u001b[94mimport\u001b[0m optim                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m13 \u001b[94mfrom\u001b[0m \u001b[4;96m.\u001b[0m\u001b[4;96mapply_optimizer_in_backward\u001b[0m \u001b[94mimport\u001b[0m (                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m\u001b[2m│   \u001b[0m_apply_optimizer_in_backward,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m│   \u001b[0m_get_in_backward_optimizers,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m)                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/distributed/optim/\u001b[0m\u001b[1;33mapply_optimizer_in_backward.py\u001b[0m:\u001b[94m12\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  9 \u001b[0m\u001b[2m# without changing it's life-time.\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 10 \u001b[0m\u001b[2m# NOTE: Alternative is to add the meta-data as an attribute to the tensor,\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 11 \u001b[0m\u001b[2m#       but that will serialize the meta-data if Tensor is serialized.\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 12 param_to_optim_hook_handle_map = torch.utils.weak.WeakTensorKeyDictionary()                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 13 \u001b[0mparam_to_acc_grad_map = torch.utils.weak.WeakTensorKeyDictionary()                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 14 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 15 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0mmodule \u001b[32m'torch.utils'\u001b[0m has no attribute \u001b[32m'weak'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dad6e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0, # 학습에 lora 적용시 rank 수\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None # 토크나이저\n",
    "                ) -> None:\n",
    "        \n",
    "        if pretrained is not None: \n",
    "            # 지정된 pre-trained 모델 불러오기\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "            \n",
    "        elif config is not None: \n",
    "            # 지정된 config 적용해 GPT2 모델 생성\n",
    "            model = GPT2Model(config)\n",
    "            \n",
    "        else:\n",
    "            # 기본 설정으로 GPT2 모델 생성\n",
    "            model = GPT2Model(GPT2Config())\n",
    "            \n",
    "        if checkpoint:\n",
    "            # checkpoint 불러오기\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        # reward가 스칼라값으로 나오도록 Head 설정\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        \"\"\"\n",
    "        학습한 Reward Model 가중치 저장하기\n",
    "        \"\"\"\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d42d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "# RM 불러오기\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93685bc",
   "metadata": {},
   "source": [
    "### 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea984a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HOME_DIR + '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    # 둘 중 랭킹이 높은 (값이 낮은) 답변을 chosen으로 저장\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    # 이하 동문\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    # 이하 동문\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "# RM 데이터셋 가공 전 크기\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "# RM 데이터셋 가공 후 크기\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "# RM 데이터셋 가공된 샘플\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bdf12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "# train/test 데이터셋 크기\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "# RewardDataset으로 Wrapping\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3469171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RewardDataset 확인하기 \n",
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)\n",
    "\n",
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained('aiffel/KoChatGPT/output_2_RM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb03cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c22678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 모델 학습을 위해 할당한 GPU 메모리 초기화\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c270c288",
   "metadata": {},
   "source": [
    "### PPO 강화학습 !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f020f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8dd71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    # actor, 문장을 생성할 SFT된 모델\n",
    "    actor = GPTActor(pretrained=HOME_DIR + '/aiffel/KoChatGPT/output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    # critic, 생성된 문장을 reward 값으로 평가할 RM\n",
    "    critic = GPTCritic(pretrained=HOME_DIR + '/aiffel/KoChatGPT/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    # 토크나이저\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    # 비교군 모델\n",
    "    initial_model = deepcopy(actor)\n",
    "    # PPO로 학습할 모델\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor  옵티마이저\n",
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "# critic 옵티마이저\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)\n",
    "\n",
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a366e2",
   "metadata": {},
   "source": [
    "### PPO 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HOME_DIR + '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=8, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "model.save_pretrained('aiffel/KoChatGPT/output_3_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답변 생성 함수\n",
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "# 지시문\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# 테스트용 질문 문장\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "# 질문 문장에 지시문 추가\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "# 질문에 따른 답변 생성\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
